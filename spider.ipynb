{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Crawler for https://kunstaspekte.art with Neo4j support\n",
    "\n",
    "## The Website \n",
    "\n",
    "It calls itself an \"international exhibition announcements and artist catalogue. It contains huge amount of data related to artists, venues and events.\n",
    "It is constructed around artist, venue and event pages, all containing links to other related pages.\n",
    "\n",
    "## The Crawler\n",
    "\n",
    "It is built on Scrapy architecture. It starts with pre-programmed initial \"starting url\" list. It goes through all the initial urls, checks if there are new urls in those pages and stores them. \n",
    "For all stored urls, it makes a new request, which is put in the request que and later scraped in compliance with the settings file.\n",
    "\n",
    "For all responses that it gets, it uses a appropriate parse method to extract data.\n",
    "\n",
    "## Parsing\n",
    "\n",
    "For parsing instead of the built-in parse functions of Scrapy, this script uses Beautiful Soup 4 module for Python, which gives more freedom in navigating the responses.\n",
    "\n",
    "## Scraped Data\n",
    "\n",
    "The appropriate data found in the responses after parsing, is sent to the items pipeline, a bulit in Scrapy functionality;\n",
    "For every page category (event, person and venue) parser decides on the appropriate item class and initiates an instance of that class. After that the items pipeline class takes action and processes the items in que, in compliance with the settings file. In this program, items pipeline starts a connection with a local Neo4j database when the crawling starts. For every item which goes through the items pipeline, the category of the item is checked, Neo4j nodes are created containing the corresponding data, and also any Neo4j relationships are created for those nodes and commited to the databese.\n",
    "\n",
    "This notebook will not connect to a Neo4j database, all the necessary code is present but commented out. instead it will write the data to a csv file.   \n",
    "Although in the end of the notebook there are few cells showing the Neo4j functionality, and some data in graph form.\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "It aims to go through the code part by part and show the neo4j integration. work in progress..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell is just setting some jupyter related settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.6.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "print('python version: ')\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell is checking for requirements, if not present it installs them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapy already installed\n",
      "neo4jupyter already installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import scrapy\n",
    "    print('scrapy already installed')\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy \n",
    "try:\n",
    "    import neo4jupyter\n",
    "    print('neo4jupyter already installed')\n",
    "except:\n",
    "    !pip install neo4jupyter\n",
    "    import neo4jupyter\n",
    "    \n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell contains the settings.py file, which is necessary for the Crawler\n",
    "#### It contains settings as;\n",
    "- Bot name\n",
    "- spider info\n",
    "- whether to obey or not robots.txt file\n",
    "- settings for the scrapy-proxy-pool moddule\n",
    "- settings for the scrapy-user-agents moddule\n",
    "- delays and concurrent process limits\n",
    "- and other Scrapy related settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "BOT_NAME = 'artist'\n",
    "\n",
    "SPIDER_MODULES = ['artist.spiders']\n",
    "NEWSPIDER_MODULE = 'artist.spiders'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "PROXY_POOL_ENABLED = True\n",
    "\n",
    "CONCURRENT_REQUESTS = 100\n",
    "\n",
    "DOWNLOAD_DELAY = 0.1\n",
    "\n",
    "RANDOM_UA_TYPE = 'desktop.random'\n",
    "RANDOM_UA_FALLBACK = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'\n",
    "RANDOM_UA_SAME_OS_FAMILY = True\n",
    "\n",
    "PROXY_POOL_FILTER_ANONYMOUS = False\n",
    "PROXY_POOL_FILTER_TYPES = ['http', 'https']\n",
    "PROXY_POOL_FILTER_CODE = 'us'\n",
    "PROXY_POOL_REFRESH_INTERVAL = 600\n",
    "PROXY_POOL_CLOSE_SPIDER = False\n",
    "PROXY_POOL_FORCE_REFRESH = False\n",
    "PROXY_POOL_TRY_WITH_HOST = True\n",
    "PROXY_POOL_PAGE_RETRY_TIMES = 3\n",
    "\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 800,\n",
    "\n",
    "    'scrapy_proxy_pool.middlewares.ProxyPoolMiddleware': 610,\n",
    "    'scrapy_proxy_pool.middlewares.BanDetectionMiddleware': 620,\n",
    "}\n",
    "\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "   'artist.pipelines.ArtistPipeline': 300,\n",
    "}\n",
    "\n",
    "# RetryMiddleware settings\n",
    "RETRY_ENABLED = True\n",
    "RETRY_TIMES = 5\n",
    "RETRY_HTTP_CODES = [500, 503, 408, 400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell contains the pipelines.py file, which is necessary for the Crawler\n",
    "It contains a pipeline method for the spider, which takes the created item objects by the parser as an argument and processes them according to their type  \n",
    "There are 3 types of items;\n",
    "- person\n",
    "- envet \n",
    "- venue\n",
    "\n",
    "#### if it is a person item\n",
    "- it creates a node of that person with all the available data as attributes and 'person' as a label and puts it in a local subgraph variable\n",
    "- for the venue list which come with the item;\n",
    " - it iterates over those list and creates new nodes with 'venue' label and only a 'url' attribute\n",
    " - then it creates a relationship between the person node and venue node with the label of 'has an exhibition in\" \n",
    "- for the event list which come with the item;\n",
    " - it iterates over those list and creates new nodes with 'event' label and only a 'url' attribute\n",
    " - then it creates a relationship between the person node and event node with the label of 'participated in\" \n",
    "- for every node and relationship created by this item, it appends them in the local subgraph\n",
    "- After going through all the data contained in the item and storing it as nodes or relationships in the subgraph it merges this subgraph with the Neo4j database\n",
    "\n",
    "#### if it is a venue item\n",
    "- it creates a node of that person with all the available data as attributes and 'venue' as a label and puts it in a local subgraph variable\n",
    "- for the artist list which come with the item;\n",
    " - it iterates over those list and creates new nodes with 'person' label and only a 'url' attribute\n",
    " - then it creates a relationship between the person node and artist node with the label of 'has an exhibition in\" \n",
    "- for the event list which come with the item;\n",
    " - it iterates over those list and creates new nodes with 'event' label and only a 'url' attribute\n",
    " - then it creates a relationship between the venue node and event node with the label of 'took place in\" \n",
    "- for the connected_venue list which come with the item;\n",
    " - it iterates over those list and creates new nodes with 'venue' label and only a 'url' attribute\n",
    " - then it creates a relationship between the venue node and the new venue node with the label of 'is cooperating with\" \n",
    "- for every node and relationship created by this item, it appends them in the local subgraph\n",
    "- After going through all the data contained in the item and storing it as nodes or relationships in the subgraph it merges this subgraph with the Neo4j database\n",
    "\n",
    "#### if it is a event item\n",
    "- it creates a node of that event with all the available data as attributes and 'event' as a label and puts it in a local subgraph variable\n",
    "- for the artist list which come with the item;\n",
    " - it iterates over those list and creates new nodes with 'person' label and only a 'url' attribute\n",
    " - then it creates a relationship between the person node and event node with the label of 'participated in\" \n",
    "- for the venue which come with the item;\n",
    " - it creates a new node with 'venue' label and only a 'url' attribute\n",
    " - then it creates a relationship between the event node and venue node with the label of 'took place in\" \n",
    "- for every node and relationship created by this item, it appends them in the local subgraph\n",
    "- After going through all the data contained in the item and storing it as nodes or relationships in the subgraph it merges this subgraph with the Neo4j database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "# defines pipeline elements\n",
    "\n",
    "from py2neo import Node, Relationship\n",
    "from py2neo import Graph, Schema\n",
    "\n",
    "\n",
    "class ArtistPipeline(object):\n",
    "\n",
    "    person_list = []\n",
    "    venue_list = []\n",
    "    event_list = []\n",
    "    \n",
    "    main_graph = Node('person') # main graph to append all the subgraphs, in order to show the neo4j functionality in jupyter notebooks\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "\n",
    "        # transaction = self.graph.begin()\n",
    "        # item_type = item.get('category')\n",
    "\n",
    "        if item_type == 'person':\n",
    "            person = Node(item.get('category'),\n",
    "                          url=item.get('url'),\n",
    "                          name=item.get('name'),\n",
    "                          bio=item.get('bio'),\n",
    "                          scrape_time=item.get('scrape_time'))\n",
    "            person.__primarylabel__ = 'person'\n",
    "            person.__primarykey__ = 'url'\n",
    "            sub_graph = person\n",
    "\n",
    "            for venues in item.get('venue_list'):\n",
    "                venue = Node('venue', url=venues)\n",
    "                venue.__primarylabel__ = 'venue'\n",
    "                venue.__primarykey__ = 'url'\n",
    "                sub_graph = sub_graph | venue\n",
    "                sub_graph = sub_graph | Relationship(person, 'has an exhibition in', venue)\n",
    "\n",
    "            for events in item.get('event_list'):\n",
    "                event = Node('event', url=events)\n",
    "                event.__primarylabel__ = 'event'\n",
    "                event.__primarykey__ = 'url'\n",
    "                sub_graph = sub_graph | event\n",
    "                sub_graph = sub_graph | Relationship(person, 'participated in', event)\n",
    "\n",
    "            # transaction.merge(sub_graph)\n",
    "            self.main_graph = main_graph | sub_graph # main graph to append all the subgraphs, in order to show the neo4j functionality in jupyter notebooks\n",
    "            print('person item')\n",
    "\n",
    "        elif item_type == 'event':\n",
    "            event = Node(item.get('category'),\n",
    "                         url=item.get('url'),\n",
    "                         name=item.get('name'),\n",
    "                         start_date=item.get('url'),\n",
    "                         end_date=item.get('url'),\n",
    "                         press_release=item.get('url'),\n",
    "                         scrape_time=item.get('scrape_time'))\n",
    "            event.__primarylabel__ = 'event'\n",
    "            event.__primarykey__ = 'url'\n",
    "            sub_graph = event\n",
    "\n",
    "            venue = Node('venue', url=item.get('venue'))\n",
    "            venue.__primarylabel__ = 'venue'\n",
    "            venue.__primarykey__ = 'url'\n",
    "\n",
    "            sub_graph = sub_graph | venue\n",
    "            sub_graph = sub_graph | Relationship(event, 'took place in', venue)\n",
    "\n",
    "            for participants in item.get('participant_list'):\n",
    "                participant = Node('person', url=participants)\n",
    "                participant.__primarylabel__ = 'person'\n",
    "                participant.__primarykey__ = 'url'\n",
    "                sub_graph = sub_graph | participant\n",
    "                sub_graph = sub_graph | Relationship(participant, 'participated in', event)\n",
    "\n",
    "            # transaction.merge(sub_graph)\n",
    "            self.main_graph = main_graph | sub_graph # main graph to append all the subgraphs, in order to show the neo4j functionality in jupyter notebooks\n",
    "            print('event item')\n",
    "\n",
    "        elif item_type == 'venue':\n",
    "            venue = Node(item.get('category'),\n",
    "                         url=item.get('url'),\n",
    "                         name=item.get('name'),\n",
    "                         city=item.get('city'),\n",
    "                         latitude=item.get('latitude'),\n",
    "                         longitude=item.get('longitude'),\n",
    "                         street_address=item.get('street_address'),\n",
    "                         website=item.get('website'),\n",
    "                         email=item.get('email'),\n",
    "                         scrape_time=item.get('scrape_time'))\n",
    "            venue.__primarylabel__ = 'venue'\n",
    "            venue.__primarykey__ = 'url'\n",
    "            sub_graph = venue\n",
    "\n",
    "            for venues in item.get('connected_venue_list'):\n",
    "                connected_venue = Node('venue', url=venues)\n",
    "                connected_venue.__primarylabel__ = 'venue'\n",
    "                connected_venue.__primarykey__ = 'url'\n",
    "                sub_graph = sub_graph | connected_venue\n",
    "                sub_graph = sub_graph | Relationship(connected_venue, 'is cooperating with', venue)\n",
    "\n",
    "            for events in item.get('event_list'):\n",
    "                event = Node('event', url=events)\n",
    "                event.__primarylabel__ = 'event'\n",
    "                event.__primarykey__ = 'url'\n",
    "                sub_graph = sub_graph | event\n",
    "                sub_graph = sub_graph | Relationship(event, 'took place in', venue)\n",
    "\n",
    "            for artists in item.get('artist_list'):\n",
    "                artist = Node('person', url=artists)\n",
    "                artist.__primarylabel__ = 'person'\n",
    "                artist.__primarykey__ = 'url'\n",
    "                sub_graph = sub_graph | artist\n",
    "                sub_graph = sub_graph | Relationship(artist, 'has an exhibition in', venue)\n",
    "\n",
    "            # transaction.merge(sub_graph)\n",
    "            self.main_graph = main_graph | sub_graph # main graph to append all the subgraphs, in order to show the neo4j functionality in jupyter notebooks\n",
    "            print('venue item')\n",
    "\n",
    "        else:\n",
    "            print('invalid item')\n",
    "\n",
    "        # transaction.commit()\n",
    "        return item\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # self.graph = Graph(\"bolt://localhost:7687\", auth=('neo4j', '123456'))\n",
    "        # self.graph.schema.create_uniqueness_constraint(\"person\", \"url\")\n",
    "        # self.graph.schema.create_uniqueness_constraint(\"venue\", \"url\")\n",
    "        # self.graph.schema.create_uniqueness_constraint(\"event\", \"url\")\n",
    "        pass\n",
    "        \n",
    "    def close_spider(self, spider):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell contains the middlewares.py file, which is necessary for the Crawler\n",
    "#### It defines middlewares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# middlewares.py\n",
    "# defines middlewares\n",
    "\n",
    "from scrapy import signals\n",
    "\n",
    "\n",
    "class ArtistSpiderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, dict or Item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Response, dict\n",
    "        # or Item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class ArtistDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell contains the items.py file, which is necessary for the Crawler\n",
    "#### It defines item classes\n",
    "\n",
    "There are 3 item classes, containing different data fields;\n",
    "- PersonItem\n",
    " - category\n",
    " - url\n",
    " - name\n",
    " - bio\n",
    " - venue_list\n",
    " - event_list\n",
    " - scrape_time\n",
    "- EventItem\n",
    " - category\n",
    " - url\n",
    " - name\n",
    " - venue\n",
    " - start_date\n",
    " - end_date\n",
    " - press_release\n",
    " - participant_list\n",
    " - scrape_time\n",
    "- VenueItem\n",
    " - category\n",
    " - url\n",
    " - name\n",
    " - connected_venue_list\n",
    " - event_list\n",
    " - artist_list\n",
    " - city\n",
    " - latitude\n",
    " - longitude\n",
    " - street_address\n",
    " - website\n",
    " - email\n",
    " - scrape_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.py\n",
    "# defines item classes\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class PersonItem(scrapy.Item):\n",
    "    category = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    bio = scrapy.Field()\n",
    "    venue_list = scrapy.Field()\n",
    "    event_list = scrapy.Field()\n",
    "    scrape_time = scrapy.Field()\n",
    "\n",
    "\n",
    "class EventItem(scrapy.Item):\n",
    "    category = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    venue = scrapy.Field()\n",
    "    start_date = scrapy.Field()\n",
    "    end_date = scrapy.Field()\n",
    "    press_release = scrapy.Field()\n",
    "    participant_list = scrapy.Field()\n",
    "    scrape_time = scrapy.Field()\n",
    "\n",
    "\n",
    "class VenueItem(scrapy.Item):\n",
    "    category = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    connected_venue_list = scrapy.Field()\n",
    "    event_list = scrapy.Field()\n",
    "    artist_list = scrapy.Field()\n",
    "    city = scrapy.Field()\n",
    "    latitude = scrapy.Field()\n",
    "    longitude = scrapy.Field()\n",
    "    street_address = scrapy.Field()\n",
    "    website = scrapy.Field()\n",
    "    email = scrapy.Field()\n",
    "    scrape_time = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell contains the artist_spider.py file, which contains the spider definition\n",
    "It contains the spider class which contains the parse and start_requests methods\n",
    "\n",
    "#### ArtistSpider()\n",
    "- Spider class definition\n",
    "- contains all related methods\n",
    "- contains spider name (necessary) and the domain allowed to scrape (not necessary)\n",
    "\n",
    "#### ArtistSpider.start_requests()\n",
    "- handles the initial request,\n",
    "- contains the start url of the artist index and a pagination list, for all the letters\n",
    " - it appends the pagination list one by one and issues a request with the 'parse_initial' parser\n",
    " - example link https://kunstaspekte.art/artists-overview/a\n",
    "- It issues for all the artist pages in the artist index\n",
    "\n",
    "#### ArtistSpider.parse_initial()\n",
    "- checks for all the links in the artist index\n",
    "- issues requests for those links with the 'parse' parser\n",
    "\n",
    "#### ArtistSpider.parse()\n",
    "- checks if its a valid page\n",
    "- decides what is the category of the page\n",
    "- sends the response to the appropriate parser;\n",
    " - event_parse\n",
    " - person_parse\n",
    " - venue_ parse\n",
    "\n",
    "#### ArtistSpider.event_parse(), ArtistSpider.venue_parse(), ArtistSpider.person_parse()\n",
    "- creates an Item\n",
    "- tries to scrape all appropriate data, if none leaves the field blank\n",
    "- sends the item to the ItemPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import scrapy\n",
    "# from ..items import PersonItem, VenueItem, EventItem\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "class ArtistSpider(scrapy.Spider):\n",
    "    name = \"artist\"\n",
    "    domain = 'https://kunstaspekte.art'\n",
    "    \n",
    "    scrape_all = 0\n",
    "\n",
    "    def start_requests(self):\n",
    "        \n",
    "        # for demonstration purposes complete crawl is disabled, instead the spider will crawl just the url list below\n",
    "        # if you want to try the complete crawl comment the urls variable and the for loop below and uncomment the 5 commented lines below. and change scrape_all variable above to 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        urls = ['https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event',\n",
    "                'https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf',\n",
    "                'https://kunstaspekte.art/venue/biennial-of-graphic-arts-ljubljana-venue',\n",
    "                'https://kunstaspekte.art/person/michel-blazy',\n",
    "                'https://kunstaspekte.art/person/index-books-peter-gidal',\n",
    "                'https://kunstaspekte.art/person/shannon-ebner'\n",
    "                ]\n",
    "        for url in urls:\n",
    "            print(url)\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "        # page_list = {'0', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        #              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}\n",
    "        # url = 'https://kunstaspekte.art/artists-overview/'\n",
    "        # for char in page_list:\n",
    "        #     yield scrapy.Request(url=url + char, callback=self.parse_initial)\n",
    "\n",
    "    def parse_initial(self, response):\n",
    "\n",
    "        raw = response.body\n",
    "        soup = BeautifulSoup(raw, 'html.parser')\n",
    "        url_list = soup.find_all('a')\n",
    "\n",
    "        for urls in url_list:\n",
    "            yield scrapy.Request(url=self.domain + urls['href'], callback=self.parse)\n",
    "            print(self.domain + urls['href'])\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        print(response)\n",
    "        raw = response.body\n",
    "        soup = BeautifulSoup(raw, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            page_type = soup.find(class_='content-heading').find('h3').get_text(' ', strip=True)\n",
    "            print(page_type)\n",
    "        except AttributeError:\n",
    "            page_type = 0\n",
    "            print(\"An exception occurred\")\n",
    "\n",
    "        if page_type == 'artist / curator':\n",
    "            yield from self.person_parse(response)\n",
    "            print('person page')\n",
    "        elif page_type == 'venue':\n",
    "            yield from self.venue_parse(response)\n",
    "            print('venue page')\n",
    "        elif page_type == 'exhibition':\n",
    "            print('exhibition page')\n",
    "            yield from self.event_parse(response)\n",
    "        else:\n",
    "            print('non valid page')\n",
    "\n",
    "    def event_parse(self, response):\n",
    "\n",
    "        print('event scraper')\n",
    "        event = EventItem()\n",
    "        # category = scrapy.Field()\n",
    "        # url = scrapy.Field()\n",
    "        # name = scrapy.Field()\n",
    "        # venue = scrapy.Field()\n",
    "        # start_date = scrapy.Field()\n",
    "        # end_date = scrapy.Field()\n",
    "        # press_release = scrapy.Field()\n",
    "        # participant_list = scrapy.Field()\n",
    "\n",
    "        category = 'event'\n",
    "\n",
    "        url = response.url\n",
    "\n",
    "        raw = response.body\n",
    "        soup = BeautifulSoup(raw, 'html.parser')\n",
    "\n",
    "        name = ''\n",
    "        try:\n",
    "            name = soup.find('h1').get_text(' ', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        venue = ''\n",
    "        try:\n",
    "            venue = self.domain + soup.find(class_='venue-module').find('h3').find('a')['href']\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        start_date = ''\n",
    "        try:\n",
    "            start_date = soup.find(class_='begins').get_text(' ', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        end_date = ''\n",
    "        try:\n",
    "            end_date = soup.find(class_='ends').get_text(' ', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        press_release = ''\n",
    "        try:\n",
    "            text = soup.find(id='textblock').find_all('p')\n",
    "            for i in text:\n",
    "                press_release = press_release + i.get_text('\\n', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        participant_list = []\n",
    "        try:\n",
    "            participants = soup.find_all(class_='artist-list')\n",
    "            for i in participants:\n",
    "                links = i.find_all('a')\n",
    "                for j in links:\n",
    "                    participant_list.append(self.domain + j['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        event['category'] = category\n",
    "        event['url'] = url\n",
    "        event['name'] = name\n",
    "        event['venue'] = venue\n",
    "        event['start_date'] = start_date\n",
    "        event['end_date'] = end_date\n",
    "        event['press_release'] = press_release\n",
    "        event['participant_list'] = participant_list\n",
    "        event['scrape_time'] = datetime.datetime.now()\n",
    "\n",
    "        yield scrapy.Request(url=venue, callback=self.parse)\n",
    "        \n",
    "        if scrape_all:\n",
    "            for url in participant_list:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "        yield event\n",
    "\n",
    "    def venue_parse(self, response):\n",
    "\n",
    "        print('venue scraper')\n",
    "        venue = VenueItem()\n",
    "        # category = scrapy.Field()\n",
    "        # url = scrapy.Field()\n",
    "        # name = scrapy.Field()\n",
    "        # connected_venue_list = scrapy.Field()\n",
    "        # event_list = scrapy.Field()\n",
    "        # artist_list = scrapy.Field()\n",
    "        # city = scrapy.Field()\n",
    "        # coordinates = scrapy.Field()\n",
    "        # street_address = scrapy.Field()\n",
    "        # website = scrapy.Field()\n",
    "        # email = scrapy.Field()\n",
    "\n",
    "        category = 'venue'\n",
    "\n",
    "        url = response.url\n",
    "\n",
    "        raw = response.body\n",
    "        soup = BeautifulSoup(raw, 'html.parser')\n",
    "\n",
    "        name = ''\n",
    "        try:\n",
    "            name = soup.find('h1').get_text(' ', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        connected_venue_list = []\n",
    "        try:\n",
    "            dependencies = soup.find(id='texts').find_all('a')\n",
    "            for i in dependencies:\n",
    "                connected_venue_list.append(self.domain + i['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        event_list = []\n",
    "        try:\n",
    "            exhibition = soup.find_all(class_='exhib-title')\n",
    "            for links in exhibition:\n",
    "                event_list.append(self.domain + links['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        artist_list = []\n",
    "        try:\n",
    "            artists = soup.find(class_='artist-list').find_all('a')\n",
    "            for links in artists:\n",
    "                artist_list.append(self.domain + links['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        city = ''\n",
    "        coordinates = ['', '']\n",
    "        street_address = ''\n",
    "        website = ''\n",
    "        mail = ''\n",
    "        try:\n",
    "            address = soup.find('div', class_='address')\n",
    "            city = address.find('p').find('a').get_text(' ', strip=True)\n",
    "            coordinates = address['data-latlon'].split(',')\n",
    "            street_address = address.find('p').get_text(' ', strip=True)\n",
    "            website = address.find(class_='website')['href']\n",
    "            mail = address.find(class_='mail')['href'].split(':')[1]\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        venue['category'] = category\n",
    "        venue['url'] = url\n",
    "        venue['name'] = name\n",
    "        venue['connected_venue_list'] = connected_venue_list\n",
    "        venue['event_list'] = event_list\n",
    "        venue['artist_list'] = artist_list\n",
    "        venue['city'] = city\n",
    "        venue['latitude'] = coordinates[0]\n",
    "        venue['longitude'] = coordinates[1]\n",
    "        venue['street_address'] = street_address\n",
    "        venue['website'] = website\n",
    "        venue['email'] = mail\n",
    "        venue['scrape_time'] = datetime.datetime.now()\n",
    "        \n",
    "        if scrape_all:\n",
    "\n",
    "            for url in connected_venue_list:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "            for url in event_list:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "            for url in artist_list:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "        yield venue\n",
    "\n",
    "    def person_parse(self, response):\n",
    "\n",
    "        print('person scraper')\n",
    "        person = PersonItem()\n",
    "        # category = scrapy.Field()\n",
    "        # url = scrapy.Field()\n",
    "        # name = scrapy.Field()\n",
    "        # bio = scrapy.Field()\n",
    "        # venue_list = scrapy.Field()\n",
    "        # event_list = scrapy.Field()\n",
    "\n",
    "        category = 'person'\n",
    "\n",
    "        url = response.url\n",
    "        print(url)\n",
    "\n",
    "        raw = response.body\n",
    "        soup = BeautifulSoup(raw, 'html.parser')\n",
    "\n",
    "        name = ''\n",
    "        try:\n",
    "            name = soup.find('h1').get_text(' ', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        bio = ''\n",
    "        try:\n",
    "            for i in soup.find_all('p'):\n",
    "                bio = bio + ' ' + i.get_text(' ', strip=True)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        venue_list = []\n",
    "        try:\n",
    "            collections = soup.find('div', class_='collections').find_all('a')\n",
    "            for collection in collections:\n",
    "                venue_list.append(self.domain + collection['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        try:\n",
    "            galleries = soup.find('div', class_='galleries').find_all('a')\n",
    "            for gallery in galleries:\n",
    "                venue_list.append(self.domain + gallery['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        event_list = []\n",
    "        try:\n",
    "            events = soup.findAll(class_='exhib-title')\n",
    "            for event in events:\n",
    "                event_list.append(self.domain + event['href'])\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        person['category'] = category\n",
    "        person['url'] = url\n",
    "        person['name'] = name\n",
    "        person['bio'] = bio\n",
    "        person['venue_list'] = venue_list\n",
    "        person['event_list'] = event_list\n",
    "        person['scrape_time'] = datetime.datetime.now()\n",
    "        \n",
    "        if scrape_all:\n",
    "            for url in venue_list:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "            for url in event_list:\n",
    "                yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "        yield person\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To try the spider out run the cell below\n",
    "\n",
    "This is not the conventional way to run a spider. Normally, for full functionality you would run it from a terminal in the project directory with the following command\n",
    "\n",
    "scrapy crawl your_spiders_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-02 15:57:02 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-02 15:57:02 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0\n",
      "2019-05-02 15:57:02 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-05-02 15:57:02 [scrapy.extensions.telnet] INFO: Telnet Password: 6ed766fc7019c22e\n",
      "2019-05-02 15:57:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-02 15:57:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-02 15:57:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-02 15:57:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-02 15:57:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-02 15:57:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-02 15:57:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x21da18a4198>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event\n",
      "https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf\n",
      "https://kunstaspekte.art/venue/biennial-of-graphic-arts-ljubljana-venue\n",
      "https://kunstaspekte.art/person/michel-blazy\n",
      "https://kunstaspekte.art/person/index-books-peter-gidal\n",
      "https://kunstaspekte.art/person/shannon-ebner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/person/index-books-peter-gidal> (referer: None)\n",
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/person/shannon-ebner> (referer: None)\n",
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/venue/biennial-of-graphic-arts-ljubljana-venue> (referer: None)\n",
      "2019-05-02 15:57:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/person/index-books-peter-gidal> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 64, in parse\n",
      "    yield from self.person_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 313, in person_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/person/shannon-ebner> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 64, in parse\n",
      "    yield from self.person_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 313, in person_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/venue/biennial-of-graphic-arts-ljubljana-venue> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 67, in parse\n",
      "    yield from self.venue_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 237, in venue_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/person/michel-blazy> (referer: None)\n",
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event> (referer: None)\n",
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf> (referer: None)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://kunstaspekte.art/person/index-books-peter-gidal>\n",
      "artist / curator\n",
      "person scraper\n",
      "https://kunstaspekte.art/person/index-books-peter-gidal\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "<200 https://kunstaspekte.art/person/shannon-ebner>\n",
      "artist / curator\n",
      "person scraper\n",
      "https://kunstaspekte.art/person/shannon-ebner\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "<200 https://kunstaspekte.art/venue/biennial-of-graphic-arts-ljubljana-venue>\n",
      "venue\n",
      "venue scraper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-05-02 15:57:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/person/michel-blazy> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 64, in parse\n",
      "    yield from self.person_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 313, in person_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://kunstaspekte.art/person/michel-blazy>\n",
      "artist / curator\n",
      "person scraper\n",
      "https://kunstaspekte.art/person/michel-blazy\n",
      "<200 https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event>\n",
      "exhibition\n",
      "exhibition page\n",
      "event scraper\n",
      "<200 https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf>\n",
      "exhibition\n",
      "exhibition page\n",
      "event scraper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-02 15:57:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 71, in parse\n",
      "    yield from self.event_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 149, in event_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 71, in parse\n",
      "    yield from self.event_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 149, in event_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/venue/mediations-biennale-poznan-venue> (referer: https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event)\n",
      "2019-05-02 15:57:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/venue/mediations-biennale-poznan-venue> (referer: https://kunstaspekte.art/event/mediations-biennale-poznan-2010-event)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 67, in parse\n",
      "    yield from self.venue_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 237, in venue_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://kunstaspekte.art/venue/nrw-forum> (referer: https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://kunstaspekte.art/venue/mediations-biennale-poznan-venue>\n",
      "venue\n",
      "venue scraper\n",
      "<200 https://kunstaspekte.art/venue/nrw-forum>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-02 15:57:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://kunstaspekte.art/venue/nrw-forum> (referer: https://kunstaspekte.art/event/deuscthland-eine-ausstellung-von-jan-boehmermann-und-btf)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Zibir Zibzan\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 67, in parse\n",
      "    yield from self.venue_parse(response)\n",
      "  File \"<ipython-input-8-ebf8d447967e>\", line 237, in venue_parse\n",
      "    if scrape_all:\n",
      "NameError: name 'scrape_all' is not defined\n",
      "2019-05-02 15:57:04 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-02 15:57:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2332,\n",
      " 'downloader/request_count': 8,\n",
      " 'downloader/request_method_count/GET': 8,\n",
      " 'downloader/response_bytes': 46101,\n",
      " 'downloader/response_count': 8,\n",
      " 'downloader/response_status_count/200': 8,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 2, 12, 57, 4, 252933),\n",
      " 'log_count/DEBUG': 8,\n",
      " 'log_count/ERROR': 8,\n",
      " 'log_count/INFO': 9,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 8,\n",
      " 'scheduler/dequeued': 8,\n",
      " 'scheduler/dequeued/memory': 8,\n",
      " 'scheduler/enqueued': 8,\n",
      " 'scheduler/enqueued/memory': 8,\n",
      " 'spider_exceptions/NameError': 8,\n",
      " 'start_time': datetime.datetime(2019, 5, 2, 12, 57, 2, 997207)}\n",
      "2019-05-02 15:57:04 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venue\n",
      "venue scraper\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(ArtistSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var link = document.createElement(\"link\");\n",
       "\tlink.ref = \"stylesheet\";\n",
       "\tlink.type = \"text/css\";\n",
       "\tlink.href = \"https://cdnjs.cloudflare.com/ajax/libs/vis/4.8.2/vis.css\";\n",
       "\tdocument.head.appendChild(link);\n",
       "require.config({     paths: {         vis: '//cdnjs.cloudflare.com/ajax/libs/vis/4.8.2/vis.min'     } }); require(['vis'], function(vis) {  window.vis = vis; }); "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-60fff4bbe37e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"person\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"venue\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"event\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mneo4jupyter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Artist_Scraper_kunstaspekte\\lib\\site-packages\\neo4jupyter\\neo4jupyter.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(graph, options, physics, limit)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \"\"\"\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Node' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "neo4jupyter.init_notebook_mode()\n",
    "\n",
    "results = ArtistPipeline.main_graph\n",
    "options = {\"person\": \"name\", \"venue\": \"name\", \"event\": \"name\"}\n",
    "\n",
    "neo4jupyter.draw(results, options)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
